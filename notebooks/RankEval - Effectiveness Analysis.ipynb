{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Effectiveness analysis \n",
    "It comprises multiple performance tests.\n",
    "\n",
    "Following steps:\n",
    "    1. Load models and test datasets \n",
    "    2. Create metrics\n",
    "    3. Effectiveness analysis:\n",
    "        - Model performance\n",
    "        - Tree-wise performance\n",
    "        - Tree-Wise average Contribution\n",
    "        - Query-wise performance\n",
    "        - Query class performance\n",
    "        - Document graded-relevance performance\n",
    "        - Rank confusion matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Useful to reload the module without having to restart the notebook kernel\n",
    "import rankeval.analysis.effectiveness\n",
    "import rankeval.visualization.effectiveness\n",
    "import rankeval.core.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data file setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/Users/muntean/Documents/workspace/quickranktestdata'\n",
    "\n",
    "# Dataset MSN with 2 models\n",
    "data_file_msn_train = os.path.join(data_dir, \"msn1/msn1.fold1.train.5k.txt\")\n",
    "data_file_msn_valid = os.path.join(data_dir, \"msn1/msn1.fold1.vali.5k.txt\")\n",
    "data_file_msn_test = os.path.join(data_dir, \"msn1/msn1.fold1.test.5k.txt\")\n",
    "model_file_50 = os.path.join(data_dir, \"new.mart.50.xml\")\n",
    "model_file_100 = os.path.join(data_dir, \"new.lmart.100.xml\")\n",
    "\n",
    "# Dataset Istella with one model\n",
    "data_file_istella = \"/Users/muntean/Documents/workspace/quickranktestdata/tiscali/tiscali.sample.txt\"\n",
    "model_file_500 = \"/Users/muntean/Documents/workspace/quickranktestdata/tiscali/lambdamart-500-alberi-50-leaves.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading models, datasets from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading Models\n",
    "from rankeval.core.model import RTEnsemble\n",
    "\n",
    "msn_mart_50 = RTEnsemble(model_file_50, name=\"MART-50\", format=\"QuickRank\")\n",
    "msn_lmart_100 = RTEnsemble(model_file_100, name=\"MART-100\", format=\"QuickRank\")\n",
    "istella_lmart_500 = RTEnsemble(model_file_500, name=\"LMART-500\", format=\"QuickRank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Datasets\n",
    "from rankeval.core.dataset import Dataset\n",
    "\n",
    "#istella\n",
    "istella_test = Dataset.load(data_file_istella, name=\"Istella-S Test\", format=\"svmlight\")\n",
    "\n",
    "#msn\n",
    "msn_train = Dataset.load(data_file_msn_train, name=\"MSN Train\", format=\"svmlight\")\n",
    "msn_validation = Dataset.load(data_file_msn_valid, name=\"MSN Valid\", format=\"svmlight\")\n",
    "msn_test = Dataset.load(data_file_msn_test, name=\"MSN Test\", format=\"svmlight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.core.metrics import Precision, Recall, NDCG\n",
    "\n",
    "precision_10 = Precision(cutoff=10)\n",
    "recall_10 = Recall(cutoff=10)\n",
    "ndcg_10 = NDCG(cutoff=10, no_relevant_results=0.5, implementation='exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effectiveness analysis\n",
    "Evaluate the effectiveness of a set of models over several datasets and using a set of metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Model performance\n",
    "\n",
    "- Compute the model performance analysis\n",
    "- Display results\n",
    "- Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Computes the model performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.analysis.effectiveness import model_performance\n",
    "\n",
    "# Istella\n",
    "istella_model_perf = model_performance(datasets=[istella_test], \n",
    "                           models=[istella_lmart_500], \n",
    "                           metrics=[precision_10, recall_10, ndcg_10])\n",
    "print istella_model_perf\n",
    "print \n",
    "\n",
    "# MSN5k\n",
    "msn_model_perf = model_performance(datasets=[msn_test], \n",
    "                       models=[msn_mart_50, msn_lmart_100], \n",
    "                       metrics=[precision_10, recall_10, ndcg_10])\n",
    "print msn_model_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each analysis returns a **xarray.DataArray** data structure. xarray is an extension of the famous pandas package offering similar features and capabilities but supporting multi-dimensional data structures (with a number of dimension potentially much greater than 2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Display results in a tabular view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.visualization.effectiveness import pretty_print_model_performance\n",
    "\n",
    "pretty_print_model_performance(istella_model_perf)\n",
    "pretty_print_model_performance(msn_model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Plot results\n",
    "\n",
    "**plot_model_performance** takes as input:\n",
    "    - the model_performance xarray (object) for given combinations of: dataset(s), model(s) and metric(s)\n",
    "    - compare: str\n",
    "        - the allowed values are: \"models\" and \"metrics\"\n",
    "        - it allows the user to compare models or metrics\n",
    "    - show values: bool\n",
    "        - shows the actual values on the plotted bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rankeval.visualization.effectiveness import plot_model_performance\n",
    "\n",
    "# MSN\n",
    "plot_model_performance(msn_model_perf, compare=\"models\",  show_values=True)\n",
    "plot_model_performance(msn_model_perf, compare=\"metrics\", show_values=True)\n",
    "\n",
    "# Istella\n",
    "plot_model_performance(istella_model_perf) \n",
    "plot_model_performance(istella_model_perf, compare=\"metrics\", show_values=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Tree-Wise Performance\n",
    "\n",
    "- Compute the tree-wise model performance analysis\n",
    "- Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Computes tree-wise performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.analysis.effectiveness import tree_wise_performance\n",
    "\n",
    "istella_tree_wise_perf = tree_wise_performance(datasets=[istella_test], \n",
    "                           models=[istella_lmart_500], \n",
    "                           metrics=[precision_10, recall_10, ndcg_10],\n",
    "                           step=10)\n",
    "print istella_tree_wise_perf\n",
    "\n",
    "msn_tree_wise_perf = tree_wise_performance(datasets=[msn_test], \n",
    "                           models=[msn_mart_50, msn_lmart_100], \n",
    "                           metrics=[precision_10, recall_10, ndcg_10],\n",
    "                           step=10)\n",
    "print msn_tree_wise_perf\n",
    "\n",
    "#### test this later, some error when more datasets\n",
    "msn_valid_tree_wise_perf = tree_wise_performance(datasets=[msn_validation, msn_test], \n",
    "                           models=[msn_mart_50], \n",
    "                           metrics=[precision_10, recall_10, ndcg_10],\n",
    "                           step=10)\n",
    "print msn_valid_tree_wise_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Plot results\n",
    "\n",
    "**plot_tree_wise_model_performance** takes as input:\n",
    "    - the tree_wise_model_performance xarray (object) for given combinations of: dataset(s), model(s) and metric(s)\n",
    "    - compare: str\n",
    "        - the allowed values are: \"models\" and \"metrics\"\n",
    "        - it allows the user to compare models or metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.visualization.effectiveness import plot_tree_wise_model_performance\n",
    "\n",
    "plot_tree_wise_model_performance(istella_tree_wise_perf, compare = \"models\")\n",
    "plot_tree_wise_model_performance(istella_tree_wise_perf, compare = \"metrics\")\n",
    "\n",
    "\n",
    "plot_tree_wise_model_performance(msn_tree_wise_perf, compare = \"models\") \n",
    "plot_tree_wise_model_performance(msn_tree_wise_perf, compare = \"metrics\")\n",
    "\n",
    "# TRY this later after fix from Salvo for multiple datasets\n",
    "plot_tree_wise_model_performance(msn_valid_tree_wise_perf, compare = \"models\")\n",
    "plot_tree_wise_model_performance(msn_valid_tree_wise_perf, compare = \"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Tree-Wise Average Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Computes tree average contribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.analysis.effectiveness import tree_wise_performance\n",
    "\n",
    "istella_tree_contrib = tree_wise_average_contribution(datasets=[istella_test], \n",
    "                           models=[istella_lmart_500])\n",
    "print istella_tree_contrib\n",
    "\n",
    "msn_tree_contrib = tree_wise_average_contribution(datasets=[msn_test], \n",
    "                           models=[msn_mart_50, msn_lmart_100])\n",
    "print msn_tree_contrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Plot results\n",
    "\n",
    "**plot_tree_wise_average_contribution** takes as input:\n",
    "    - the tree_wise_average_contribution xarray (object) for given combinations of: dataset(s), model(s) and metric(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.visualization.effectiveness import plot_tree_wise_average_contribution\n",
    "\n",
    "plot_tree_wise_average_contribution(istella_tree_contrib)\n",
    "plot_tree_wise_average_contribution(msn_tree_contrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query-Wise Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Computes query wise performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.analysis.effectiveness import query_wise_performance\n",
    "\n",
    "msn_query_wise_perf = query_wise_performance(datasets=[msn_test], \n",
    "                            models=[msn_mart_50, msn_lmart_100], \n",
    "                            metrics=[precision_10, recall_10, ndcg_10],\n",
    "                            bins=50)\n",
    "print msn_query_wise_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Plot results\n",
    "\n",
    "**plot_query_wise_performance** takes as input:\n",
    "    - the query_wise_performance xarray (object) for given combinations of: dataset(s), model(s) and metric(s)\n",
    "    - compare: str\n",
    "        - the allowed values are: \"models\" and \"metrics\"\n",
    "        - it allows the user to compare models or metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.visualization.effectiveness import plot_query_wise_performance\n",
    "\n",
    "plot_query_wise_performance(msn_query_wise_perf, compare=\"models\")\n",
    "plot_query_wise_performance(msn_query_wise_perf, compare=\"metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Graded Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Computes document graded relevance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.analysis.effectiveness import document_graded_relevance\n",
    "\n",
    "istella_doc_graded_rel = document_graded_relevance(datasets=[istella_test], \n",
    "                               models=[istella_lmart_500], \n",
    "                               bins=50)\n",
    "print istella_doc_graded_rel\n",
    "\n",
    "msn_doc_graded_rel = document_graded_relevance(datasets=[msn_test], \n",
    "                                models=[msn_mart_50, msn_lmart_100],  \n",
    "                                bins=50)\n",
    "print msn_doc_graded_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Plot results\n",
    "\n",
    "**plot_document_graded_relevance** takes as input:\n",
    "    - the document_graded_relevance xarray (object) for given combinations of: dataset(s), model(s) and metric(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.visualization.effectiveness import plot_document_graded_relevance\n",
    "\n",
    "plot_document_graded_relevance(istella_doc_graded_rel) \n",
    "plot_document_graded_relevance(msn_doc_graded_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rank-Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Computes the rank confusion matrix analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.analysis.effectiveness import rank_confusion_matrix\n",
    "\n",
    "istella_confusion_matrix = rank_confusion_matrix(datasets=[istella_test], \n",
    "                               models=[istella_lmart_500])\n",
    "print istella_confusion_matrix\n",
    "\n",
    "msn_confusion_matrix = rank_confusion_matrix(datasets=[msn_test], \n",
    "                                models=[msn_mart_50, msn_lmart_100])\n",
    "print msn_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Plot results\n",
    "\n",
    "**plot_rank_confusion_matrix** takes as input:\n",
    "    - the rank_confusion_matrix xarray (object) for given combinations of: dataset(s), model(s) and metric(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.visualization.effectiveness import plot_rank_confusion_matrix\n",
    "\n",
    "plot_rank_confusion_matrix(df) \n",
    "plot_rank_confusion_matrix(df_msn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query-Class Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Computes the query class performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.analysis.effectiveness import query_class_performance\n",
    "\n",
    "classes = ['A', 'B', 'C', 'D', 'E']\n",
    "query_classes = np.array([[classes[np.random.randint(len(classes))] \n",
    "                           for _ in range(istella.n_queries)]])\n",
    "\n",
    "istella_query_class_perf = query_class_performance(datasets=[istella_test], \n",
    "                             models=[istella_lmart_500], \n",
    "                             metrics=[precision_10, recall_10, ndcg_10],\n",
    "                             query_classes=query_classes)\n",
    "print istella_query_class_perf\n",
    "\n",
    "msn_query_class_perf = query_class_performance(datasets=[msn_test], \n",
    "                                 models=[msn_mart_50, msn_lmart_100],\n",
    "                                 metrics=[precision_10, recall_10, ndcg_10],\n",
    "                                 query_classes=query_classes)\n",
    "print msn_query_class_perf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Plot results\n",
    "\n",
    "**plot_query_class_performance** takes as input:\n",
    "    - the query_class_performance xarray (object) for given combinations of: dataset(s), model(s) and metric(s)\n",
    "    - compare: str\n",
    "        - the allowed values are: \"models\" and \"metrics\"\n",
    "        - it allows the user to compare models or metrics\n",
    "    - show values: bool\n",
    "        - shows the actual values on the plotted bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.visualization.effectiveness import plot_query_class_performance\n",
    "\n",
    "plot_query_class_performance(df, compare=\"metrics\", show_values = True)\n",
    "plot_query_class_performance(df, compare=\"models\", show_values = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
