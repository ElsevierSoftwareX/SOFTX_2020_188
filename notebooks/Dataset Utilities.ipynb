{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utilities\n",
    "\n",
    "In the following notebook we will demonstrate several utilities provided by RankEval in working and manipulating a Dataset in the SVMLight format. In particular, how to:\n",
    " - Easily Load standard LtR datasets as well as several pre-trained models\n",
    " - Fork a dataset by selecting only a subset of the features\n",
    " - Fork a dataset by selecting only some queries\n",
    " - Dump a dataset on file in the SVMLight format\n",
    " - Split a dataset in train, validation (eventually) and test sets\n",
    " - Manually accessing low level dataset information\n",
    " - Iterate over each query of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Useful to reload the module without having to restart the notebook kernel\n",
    "from rankeval.dataset.datasets_fetcher import load_dataset\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.model import RTEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets and models\n",
    "\n",
    "Standard LtR datasets can be easily loaded by calling the load_dataset utility. This tool allows to load datasets and several pre-trained models from a central repository, in such a way to simplify the setting of the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files. This may take a few minutes.\n",
      "done loading dataset!\n"
     ]
    }
   ],
   "source": [
    "# Dataset container\n",
    "dataset_container = load_dataset(dataset_name='msn10k', \n",
    "                                fold='1', \n",
    "                                download_if_missing=True, \n",
    "                                force_download=False,\n",
    "                                with_models=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remapping Datasets Names\n",
    "msn_train = dataset_container.train_dataset\n",
    "msn_validation = dataset_container.validation_dataset\n",
    "msn_test = dataset_container.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /Users/salvatore/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T15000.xml\n",
      "1 /Users/salvatore/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T5000.xml\n",
      "2 /Users/salvatore/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T20000.xml\n",
      "3 /Users/salvatore/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T10000.xml\n",
      "4 /Users/salvatore/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T1000.xml\n",
      "5 /Users/salvatore/rankeval_data/msn10k/models/Fold1/xgboost/XGBOOST.msn10k.fold-1.pairwise.d-5.lr-10.trees-1000.model\n",
      "6 /Users/salvatore/rankeval_data/msn10k/models/Fold1/lightgbm/LGBM.msn10k.fold-1.lambdarank.leaves-32.lr-5.trees-1000.model\n",
      "7 /Users/salvatore/rankeval_data/msn10k/models/Fold1/catboost/msn1.catboost.LAMBDAMART.1000.5.T1000.json\n",
      "8 /Users/salvatore/rankeval_data/msn10k/models/Fold1/catboost/msn1.catboost.LAMBDAMART.1000.5.T1000.model\n"
     ]
    }
   ],
   "source": [
    "# View available models\n",
    "for item, file_name in enumerate(dataset_container.model_filenames):\n",
    "    print (item, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model files\n",
    "msn_qr_lmart_1Ktrees_file = dataset_container.model_filenames[4]\n",
    "# Loading model into RankEval\n",
    "msn_qr_lmart_1Ktrees = RTEnsemble(msn_qr_lmart_1Ktrees_file, name=\"QR_lmart_1K\", format=\"QuickRank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fork a dataset by selecting only a subset of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from a dataset, it is possible to create a new dataset instance with only a subset of the features appearing in the original dataset. \n",
    "This feature is particularly useful when the task is to analyze the feature importance, trying to reduce as much as possible the features needed by a LtR model without affecting the quality of the learned model. \n",
    "An example of such analysis is reported in the notebook: [Feature Analysis](Feature%20Analysis.ipynb).\n",
    "\n",
    "In this notebook the features selected will be the 20% of all the features, randomly choosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_permutation = np.random.permutation(msn_train.n_features)\n",
    "selected_features = feature_permutation[:int(msn_train.n_features*0.2)]\n",
    "\n",
    "msn_train_subset_features = msn_train.subset_features(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Queries</th>\n",
       "      <th># Instances</th>\n",
       "      <th># Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Full</th>\n",
       "      <td>6000</td>\n",
       "      <td>723412</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sampled by Feature</th>\n",
       "      <td>6000</td>\n",
       "      <td>723412</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    # Queries  # Instances  # Features\n",
       "Full                     6000       723412         136\n",
       "Sampled by Feature       6000       723412          27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'# Queries': [msn_train.n_queries, msn_train_subset_features.n_queries], \n",
    "     '# Instances': [msn_train.n_instances, msn_train_subset_features.n_instances], \n",
    "     '# Features': [msn_train.n_features, msn_train_subset_features.n_features],}\n",
    "df = pd.DataFrame(data=d, index=[\"Full\", \"Sampled by Feature\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the models have to be used only in consistent conditions, i.e., if you train a model on this sampled dataset also the test dataset (and eventually the validation) has to be changed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "msn_test_subset_features = msn_test.subset_features(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fork a dataset by selecting only a subset of the queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from a dataset, it is possible to create a new dataset instance by selecting only some of the queries of the original dataset. The query to select are identified by qid (query id).\n",
    "\n",
    "In this notebook we select only 20% of all the queries, randomly choosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_permutation = np.random.permutation(msn_train.query_ids)\n",
    "selected_qid = qid_permutation[:int(msn_train.query_ids.size*0.2)]\n",
    "\n",
    "msn_train_subset_queries = msn_train.subset(query_ids=selected_qid, name=\"MSN Train Fold1 - 20% of the queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Queries</th>\n",
       "      <th># Instances</th>\n",
       "      <th># Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Full</th>\n",
       "      <td>6000</td>\n",
       "      <td>723412</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sampled by Query</th>\n",
       "      <td>1200</td>\n",
       "      <td>141684</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  # Queries  # Instances  # Features\n",
       "Full                   6000       723412         136\n",
       "Sampled by Query       1200       141684         136"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'# Queries': [msn_train.n_queries, msn_train_subset_queries.n_queries], \n",
    "     '# Instances': [msn_train.n_instances, msn_train_subset_queries.n_instances], \n",
    "     '# Features': [msn_train.n_features, msn_train_subset_queries.n_features],}\n",
    "df = pd.DataFrame(data=d, index=[\"Full\", \"Sampled by Query\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump a dataset on file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you modify a dataset (by selecting only a subset of the queries or of the features) you can save the modified version on file in the standard SVMLight format. This operation is provided by RankEval to simplify the activity of manipulating and working on LtR datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_file = \"msn_fold1_27_random_features.txt\"\n",
    "msn_train_subset_features.dump(destination_file, format=\"svmlight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the top 10 lines of the file where the dataset has been written into (and delete it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 qid:1 1:2 2:0 3:0 4:0 5:0 6:116 7:1 8:0 9:1 10:6.93127489 11:-14.5185232 12:0 13:0 14:0 15:13 16:0.69472599 17:22.0769272 18:5.99245977 19:3 20:167 21:0.25 22:-7.76007223 23:0 24:-18.5677929 25:0 26:0 27:-13.5819321 \n",
      "2 qid:1 1:11 2:0 3:0 4:0.200000003 5:0 6:124 7:0 8:0 9:1 10:6.93127489 11:-11.2978106 12:1 13:1 14:0 15:1 16:61.9195633 17:22.0769272 18:0 19:2 20:416 21:0 22:-21.2421703 23:0 24:-11.55585 25:4.99999987e-06 26:1 27:-11.411068 \n",
      "0 qid:1 1:8 2:0.222222 3:0 4:0.333332986 5:0 6:124 7:0 8:0 9:1 10:6.93127489 11:-12.4879894 12:0.666666985 13:0 14:0 15:14 16:42.8794327 17:22.0769272 18:0 19:67 20:156 21:0 22:-21.2421703 23:0 24:-12.6090651 25:0.000392999995 26:1 27:-11.4363775 \n",
      "2 qid:1 1:4 2:0 3:0 4:0.25 5:0 6:123 7:0 8:0 9:1 10:6.93127489 11:-14.1989012 12:1 13:1 14:0 15:1 16:1.29454005 17:22.0769272 18:0 19:3 20:299 21:0 22:-21.2421703 23:0 24:-15.5556402 25:1.99999999e-06 26:1 27:-13.8254166 \n",
      "1 qid:1 1:4 2:0 3:0 4:0.25 5:0 6:256 7:0 8:0 9:1 10:6.93127489 11:-19.469471 12:1 13:1 14:0 15:1 16:3.62689304 17:22.0769272 18:0 19:13 20:2022 21:0 22:-21.2421703 23:0 24:-20.6738873 25:0 26:1 27:-19.2260437 \n",
      "1 qid:1 1:12 2:0 3:0 4:0.25 5:0 6:210 7:0 8:0 9:1 10:6.93127489 11:-15.0384054 12:1 13:1 14:0 15:5 16:86.3032761 17:22.0769272 18:0 19:15 20:949 21:0 22:-21.2421703 23:0 24:-15.7556438 25:1.70000003e-05 26:1 27:-14.9845409 \n",
      "1 qid:1 1:11 2:0 3:0 4:0.25 5:0 6:256 7:0 8:0 9:1 10:6.93127489 11:-16.1355724 12:1 13:1 14:0 15:3 16:117.033638 17:22.0769272 18:0 19:14 20:1378 21:0 22:-21.2421703 23:0 24:-16.8580341 25:7.0000001e-06 26:1 27:-16.0893173 \n",
      "2 qid:1 1:24 2:0 3:0 4:0.25 5:0 6:115 7:0 8:0 9:1 10:6.93127489 11:-10.938158 12:1 13:1 14:0 15:1 16:576.506775 17:22.0769272 18:0 19:3 20:503 21:0 22:-21.2421703 23:0 24:-11.2061338 25:0.000300999993 26:1 27:-11.05159 \n",
      "1 qid:1 1:8 2:0 3:0 4:0.25 5:0 6:124 7:0 8:0 9:1 10:6.93127489 11:-16.6245098 12:1 13:1 14:0 15:1 16:11.8807268 17:22.0769272 18:0 19:3 20:1308 21:0 22:-21.2421703 23:0 24:-17.2991886 25:1.99999999e-06 26:1 27:-16.5673866 \n",
      "0 qid:1 1:24 2:0 3:0 4:0.25 5:0 6:541 7:0 8:0 9:1 10:6.93127489 11:-11.0318766 12:1 13:1 14:0 15:1 16:576.506775 17:22.0769272 18:0 19:5 20:519 21:0 22:-21.2421703 23:0 24:-11.3318119 25:0.000283000001 26:1 27:-11.1454268 \n"
     ]
    }
   ],
   "source": [
    "!head -10 $destination_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "msn_train_subset_features_loaded = Dataset.load(destination_file, format=\"svmlight\", name=\"Dataset load from file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the dataset dumped on file and then re-read are exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train_subset_features_loaded == msn_train_subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm $destination_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a dataset from numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 rows, 25 features each, random generated\n",
    "n_rows = 1000\n",
    "n_features = 25\n",
    "\n",
    "X = np.random.random(size=(n_rows, n_features))\n",
    "y = np.random.randint(0, 4, size=n_rows, dtype=np.uint8)\n",
    "\n",
    "# random generator of qids, then sorted for packaging them consecutively\n",
    "qids = np.random.randint(0, 10, size=n_rows, dtype=np.uint8)\n",
    "qids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dataset = Dataset(X, y, qids, name=\"Dataset created from numpy array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split a dataset in train, validation (eventually) and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common the need to split a LtR dataset in train, validation and test sets as to train a LtR model and test its effectiveness on a different split.\n",
    "Rankeval provides an utility to split a dataset into partitions. It shuffle the query ids before partitioning.\n",
    "\n",
    "In this notebook for simplicity we split the training set of the MSN-Fold1 dataset according to the 60%/20%/20% ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vali, test = msn_train.split(train_size=0.6, vali_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Queries</th>\n",
       "      <th># Instances</th>\n",
       "      <th># Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>3600</td>\n",
       "      <td>433775</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vali</th>\n",
       "      <td>1200</td>\n",
       "      <td>145597</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>1200</td>\n",
       "      <td>144040</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       # Queries  # Instances  # Features\n",
       "Train       3600       433775         136\n",
       "Vali        1200       145597         136\n",
       "Test        1200       144040         136"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'# Queries': [train.n_queries, vali.n_queries, test.n_queries], \n",
    "     '# Instances': [train.n_instances, vali.n_instances, test.n_instances], \n",
    "     '# Features': [train.n_features, vali.n_features, test.n_features],}\n",
    "df = pd.DataFrame(data=d, index=[\"Train\", \"Vali\", \"Test\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `vali_size` is set to 0, the method will skip the creation of a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing low-level information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset class provides an interface storing dataset information and providing utilities for easily accessing it. In some situation however it is needed to access the low level information there stored. In this notebook, we describe how this information is stored, and how to iterate over the queries of a dataset accessing features, labels and query ids in order.\n",
    "\n",
    "We start by describing the base component of a dataset instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the main charateristics of the dataset we are adopting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Queries</th>\n",
       "      <th># Instances</th>\n",
       "      <th># Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <td>6000</td>\n",
       "      <td>723412</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         # Queries  # Instances  # Features\n",
       "Dataset       6000       723412         136"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'# Queries': [msn_train.n_queries], \n",
    "     '# Instances': [msn_train.n_instances], \n",
    "     '# Features': [msn_train.n_features],}\n",
    "df = pd.DataFrame(data=d, index=[\"Dataset\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is described primarily by the feature matrix (numpy 2d array) of its feature. The rows of this matrix are the instances, whether the columns are the features. The shape of the matrix is thus (n_instances, n_features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723412, 136)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell of this matrix identify the value of a single feature/document pair. For example, the value of the 10-th feature of the 5-th instance is (remember indices starts from 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.X[4, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth labels are stored in a separate vector (numpy 1d array), with a single value for each dataset instance. The shape of the vector is thus (n_instances):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723412,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell of this vector identify the ground truth value of a single instance for the specific query. For example, the ground truth value of the 5-th instance is (recall ground truth labels usually range in [0-4], with 0 meaning completely irrelevant and 4 perfectly relevant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.y[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query ids of the dataset are stored in a separate vector. This information is not strictly needed, but when you manipulate a dataset sometime it is important to preserve this information (e.g., for comparison).\n",
    "This information is stored in a vector (numpy 1d array) with a single value for each dataset query. The shape of the vector is thus (n_queries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.query_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query id of the first query is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.query_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offsets of each query are on the other hand stored in another vector (numpy 1d array). These offsets allows to associate each dataset instance to the correct query. Recall indeed that both features and ground truth labels are stored contiguosly between several queries, thus it is needed a mechanism allowing to discriminate and reconstruct the information regarding each query.\n",
    "\n",
    "This array stores the starting offset of each query (starting from the first instance identified by the first row). Its shape is thus (n_queries+1), with the latter element that is not strictly needed but is put there for simplicity (its value is always the number of instances in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6001,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.query_offsets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this information, the i-th query has indices ranging from query_offsets[i] up to query_offsets[i+1], with the latter element excluded. For example, the first query start at index 0 and contains 86 documents up to index 86 (excluded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 86])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.query_offsets[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of documents in each query can thus be obtained automatically from the `query_offset` variable. However, RankEval provide a simple utility for accessing this information, that is the `get_query_sizes` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 86, 106,  92, ...,  79, 180,  40])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.get_query_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns a vector (numpy 1d array) with a single element for each query, identifying the number of documents belonging to this query. As noticed above, the first query has 86 documents, the second has 106 documents, and so on and so forth. The shape of this vector is thus (n_queries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.get_query_sizes().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situation we need to have the list of query ids, one for each instance (in place of the compact version adopted by RankEval). For example, this feature is needed by LightGBM when you need to create a dataset. RankEval provides such information with the `get_qids_dataset` utility, returning a vector (numpy 1d array) with shape (n_instances):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723412,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.get_qids_dataset().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first document has qid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.get_qids_dataset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over each query of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime it is needed to iterate over the low level information stored in the dataset instance. RankEval allows you to do that with the `query_iterator` utility, that provides you with an iterator over the offsets of the query_ids in the dataset. In particular, each element of the iterator is a tuple (qid, start_offset, end_offset), highlighting thus the qid of the query and the row index of the instances belonging it. For example, the first element of the iterator is related to the query with `qid=1` and offset ranging from 0 to 86 (excluded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 86)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(msn_train.query_iterator())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
