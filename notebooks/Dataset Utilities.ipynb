{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utilities\n",
    "\n",
    "In the following notebook we will demonstrate several utilities provided by RankEval in working and manipulating a Dataset in the SVMLight format. In particular, how to:\n",
    " - Easily Load standard LtR datasets as well as several pre-trained models\n",
    " - Fork a dataset by selecting only a subset of the features\n",
    " - Fork a dataset by selecting only some queries\n",
    " - Dump a dataset on file in the SVMLight format\n",
    " - Split a dataset in train, validation (eventually) and test sets\n",
    " - Manually accessing low level dataset information\n",
    " - Iterate over each query of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Useful to reload the module without having to restart the notebook kernel\n",
    "from rankeval.dataset.datasets_fetcher import load_dataset\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.model import RTEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets and models\n",
    "\n",
    "Standard LtR datasets can be easily loaded by calling the load_dataset utility. This tool allows to load datasets and several pre-trained models from a central repository, in such a way to simplify the setting of the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files. This may take a few minutes.\n",
      "done loading dataset!\n"
     ]
    }
   ],
   "source": [
    "# Dataset container\n",
    "dataset_container = load_dataset(dataset_name='msn10k', \n",
    "                                fold='1', \n",
    "                                download_if_missing=True, \n",
    "                                force_download=False,\n",
    "                                with_models=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remapping Datasets Names\n",
    "msn_train = dataset_container.train_dataset\n",
    "msn_validation = dataset_container.validation_dataset\n",
    "msn_test = dataset_container.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /Users/salvatore/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T15000.xml\n",
      "1 /Users/salvatore/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T5000.xml\n",
      "2 /Users/salvatore/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T20000.xml\n",
      "3 /Users/salvatore/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T10000.xml\n",
      "4 /Users/salvatore/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T1000.xml\n",
      "5 /Users/salvatore/rankeval_data/msn10k/models/Fold1/xgboost/XGBOOST.msn10k.fold-1.pairwise.d-5.lr-10.trees-1000.model\n",
      "6 /Users/salvatore/rankeval_data/msn10k/models/Fold1/lightgbm/LGBM.msn10k.fold-1.lambdarank.leaves-32.lr-5.trees-1000.model\n",
      "7 /Users/salvatore/rankeval_data/msn10k/models/Fold1/catboost/msn1.catboost.LAMBDAMART.1000.5.T1000.json\n",
      "8 /Users/salvatore/rankeval_data/msn10k/models/Fold1/catboost/msn1.catboost.LAMBDAMART.1000.5.T1000.model\n"
     ]
    }
   ],
   "source": [
    "# View available models\n",
    "for item, file_name in enumerate(dataset_container.model_filenames):\n",
    "    print item, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model files\n",
    "msn_qr_lmart_1Ktrees_file = dataset_container.model_filenames[4]\n",
    "# Loading model into RankEval\n",
    "msn_qr_lmart_1Ktrees = RTEnsemble(msn_qr_lmart_1Ktrees_file, name=\"QR_lmart_1K\", format=\"QuickRank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fork a dataset by selecting only a subset of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from a dataset, it is possible to create a new dataset instance with only a subset of the features appearing in the original dataset. \n",
    "This feature is particularly useful when the task is to analyze the feature importance, trying to reduce as much as possible the features needed by a LtR model without affecting the quality of the learned model. \n",
    "An example of such analysis is reported in the notebook: [Feature Analysis](Feature%20Analysis.ipynb).\n",
    "\n",
    "In this notebook the features selected will be the 20% of all the features, randomly choosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_permutation = np.random.permutation(msn_train.n_features)\n",
    "selected_features = feature_permutation[:int(msn_train.n_features*0.2)]\n",
    "\n",
    "msn_train_subset_features = msn_train.subset_features(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Features</th>\n",
       "      <th># Instances</th>\n",
       "      <th># Queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Full</th>\n",
       "      <td>136</td>\n",
       "      <td>723412</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sampled by Feature</th>\n",
       "      <td>27</td>\n",
       "      <td>723412</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    # Features  # Instances  # Queries\n",
       "Full                       136       723412       6000\n",
       "Sampled by Feature          27       723412       6000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'# Queries': [msn_train.n_queries, msn_train_subset_features.n_queries], \n",
    "     '# Instances': [msn_train.n_instances, msn_train_subset_features.n_instances], \n",
    "     '# Features': [msn_train.n_features, msn_train_subset_features.n_features],}\n",
    "df = pd.DataFrame(data=d, index=[\"Full\", \"Sampled by Feature\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the models have to be used only in consistent conditions, i.e., if you train a model on this sampled dataset also the test dataset (and eventually the validation) has to be changed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "msn_test_subset_features = msn_test.subset_features(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fork a dataset by selecting only a subset of the queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from a dataset, it is possible to create a new dataset instance by selecting only some of the queries of the original dataset. The query to select are identified by qid (query id).\n",
    "\n",
    "In this notebook we select only 20% of all the queries, randomly choosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_permutation = np.random.permutation(msn_train.query_ids)\n",
    "selected_qid = qid_permutation[:int(msn_train.query_ids.size*0.2)]\n",
    "\n",
    "msn_train_subset_queries = msn_train.subset(query_ids=selected_qid, name=\"MSN Train Fold1 - 20% of the queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Features</th>\n",
       "      <th># Instances</th>\n",
       "      <th># Queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Full</th>\n",
       "      <td>136</td>\n",
       "      <td>723412</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sampled by Query</th>\n",
       "      <td>136</td>\n",
       "      <td>142441</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  # Features  # Instances  # Queries\n",
       "Full                     136       723412       6000\n",
       "Sampled by Query         136       142441       1200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'# Queries': [msn_train.n_queries, msn_train_subset_queries.n_queries], \n",
    "     '# Instances': [msn_train.n_instances, msn_train_subset_queries.n_instances], \n",
    "     '# Features': [msn_train.n_features, msn_train_subset_queries.n_features],}\n",
    "df = pd.DataFrame(data=d, index=[\"Full\", \"Sampled by Query\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump a model on file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you modify a dataset (by selecting only a subset of the queries or of the features) you can save the modified version on file in the standard SVMLight format. This operation is provided by RankEval to simplify the activity of manipulating and working on LtR datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_file = \"msn_fold1_27_random_features.txt\"\n",
    "msn_train_subset_features.dump(destination_file, format=\"svmlight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the top 10 lines of the file where the dataset has been written into (and delete it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 qid:1 1:3.078917 2:0 3:0.0064099999 4:2 5:11089534 6:0.25 7:1 8:0 9:167 10:3 11:6 12:0 13:-20.203779 14:7 15:-13.581932 16:0 17:1 18:0 19:13.853103 20:0.75 21:2 22:0.011976 23:0 24:20.59276 25:6.9265509 26:1 27:0 \n",
      "2 qid:1 1:30.789171 2:0 3:0.022988999 4:10.333333 5:11089534 6:0 7:0 8:0 9:416 10:3 11:31 12:0 13:-16.208809 14:5 15:-11.411068 16:0.2 17:0 18:0.88888901 19:70.792755 20:0 21:9 22:0.026442001 23:0 24:0 25:6.9265509 26:0 27:0 \n",
      "0 qid:1 1:18.473503 2:0 3:0.031962998 4:5.333333 5:3 6:0 7:0 8:0 9:156 10:3 11:16 12:0 13:-18.589542 14:7 15:-11.436378 16:0 17:0 18:9.5555563 19:33.436523 20:0 21:1 22:0.051282 23:0 24:0 25:6.9265509 26:0 27:0 \n",
      "2 qid:1 1:6.1578341 2:0 3:0.00813 4:3.333333 5:11089534 6:0 7:0 8:0 9:299 10:3 11:10 12:0 13:-19.180737 14:7 15:-13.825417 16:0.25 17:0 18:0.222222 19:21.928251 20:0 21:3 22:0.013378 23:0 24:0 25:6.9265509 26:0 27:0 \n",
      "1 qid:1 1:8.100687 2:0 3:0.001327 4:3.666667 5:5 6:0 7:0 8:0 9:2022 10:3 11:11 12:0 13:-20.589939 14:7 15:-19.226044 16:0.25 17:0 18:0.222222 19:24.627909 20:0 21:3 22:0.0019779999 23:0 24:0 25:6.9265509 26:0 27:0 \n",
      "1 qid:1 1:27.710255 2:0 3:0.0078429999 4:8.333333 5:6 6:0 7:0 8:0 9:949 10:3 11:25 12:0 13:-17.869366 14:7 15:-14.984541 16:0.25 17:0 18:14.888889 19:52.665119 20:0 21:3 22:0.012645 23:0 24:0 25:6.9265509 26:0 27:0 \n",
      "1 qid:1 1:30.789171 2:0 3:0.00538 4:8.333333 5:6 6:0 7:0 8:0 9:1378 10:3 11:25 12:0 13:-18.26152 14:7 15:-16.089317 16:0.25 17:0 18:14.222222 19:54.594822 20:0 21:3 22:0.007983 23:0 24:0 25:6.9265509 26:0 27:0 \n",
      "2 qid:1 1:67.736176 2:0 3:0.033401001 4:17.333332 5:0 6:0 7:0 8:0 9:503 10:3 11:52 12:0 13:-15.438229 14:10 15:-11.05159 16:0.25 17:0 18:76.222221 19:111.86548 20:0 21:5 22:0.047713999 23:0 24:0 25:6.9265509 26:0 27:0 \n",
      "1 qid:1 1:15.394586 2:0 3:0.0038610001 4:6 5:11089534 6:0 7:0 8:0 9:1308 10:3 11:18 12:0 13:-18.794758 14:7 15:-16.567387 16:0.25 17:0 18:2.666667 19:38.457195 20:0 21:4 22:0.006116 23:0 24:0 25:6.9265509 26:0 27:0 \n",
      "0 qid:1 1:67.736176 2:0 3:0.032026 4:17.333332 5:1 6:0 7:0 8:0 9:519 10:3 11:52 12:0 13:-15.457345 14:5 15:-11.145427 16:0.25 17:0 18:76.222221 19:111.86548 20:0 21:5 22:0.046243001 23:0 24:0 25:6.9265509 26:0 27:0 \n"
     ]
    }
   ],
   "source": [
    "!head -10 $destination_file\n",
    "!rm $destination_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split a dataset in train, validation (eventually) and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common the need to split a LtR dataset in train, validation and test sets as to train a LtR model and test its effectiveness on a different split.\n",
    "Rankeval provides an utility to split a dataset into partitions. It shuffle the query ids before partitioning.\n",
    "\n",
    "In this notebook for simplicity we split the training set of the MSN-Fold1 dataset according to the 60%/20%/20% ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vali, test = msn_train.split(train_size=0.6, vali_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Features</th>\n",
       "      <th># Instances</th>\n",
       "      <th># Queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>136</td>\n",
       "      <td>437968</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vali</th>\n",
       "      <td>136</td>\n",
       "      <td>142682</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>136</td>\n",
       "      <td>142762</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       # Features  # Instances  # Queries\n",
       "Train         136       437968       3600\n",
       "Vali          136       142682       1200\n",
       "Test          136       142762       1200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'# Queries': [train.n_queries, vali.n_queries, test.n_queries], \n",
    "     '# Instances': [train.n_instances, vali.n_instances, test.n_instances], \n",
    "     '# Features': [train.n_features, vali.n_features, test.n_features],}\n",
    "df = pd.DataFrame(data=d, index=[\"Train\", \"Vali\", \"Test\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `vali_size` is set to 0, the method will skip the creation of a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing low-level information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset class provides an interface storing dataset information and providing utilities for easily accessing it. In some situation however it is needed to access the low level information there stored. In this notebook, we describe how this information is stored, and how to iterate over the queries of a dataset accessing features, labels and query ids in order.\n",
    "\n",
    "We start by describing the base component of a dataset instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the main charateristics of the dataset we are adopting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Features</th>\n",
       "      <th># Instances</th>\n",
       "      <th># Queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <td>136</td>\n",
       "      <td>723412</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         # Features  # Instances  # Queries\n",
       "Dataset         136       723412       6000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'# Queries': [msn_train.n_queries], \n",
    "     '# Instances': [msn_train.n_instances], \n",
    "     '# Features': [msn_train.n_features],}\n",
    "df = pd.DataFrame(data=d, index=[\"Dataset\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is described in primis by the feature matrix (numpy 2d array) of its feature. The rows of this matrix are the instances, whether the columns are the features. The shape of the matrix is thus (n_instances, n_features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723412, 136)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell of this matrix identify the value of a single feature/document pair. For example, the value of the 10-th feature of the 5-th instance is (remember indices starts from 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.X[4, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth labels are stored in a separate vector (numpy 1d array), with a single value for each dataset instance. The shape of the vector is thus (n_instances):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723412,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell of this vector identify the ground truth value of a single instance for the specific query. For example, the ground truth value of the 5-th instance is (recall ground truth labels usually range in [0-4], with 0 meaning completely irrelevant and 4 perfectly relevant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.y[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query ids of the dataset are stored in a separate vector. This information is not strictly needed, but when you manipulate a dataset sometime it is important to preserve this information (e.g., for comparison).\n",
    "This information is stored in a vector (numpy 1d array) with a single value for each dataset query. The shape of the vector is thus (n_queries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.query_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query id of the first query is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.query_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offsets of each query are on the other hand stored in another vector (numpy 1d array). These offsets allows to associate each dataset instance to the correct query. Recall indeed that both features and ground truth labels are stored contiguosly between several queries, thus it is needed a mechanism allowing to discriminate and reconstruct the information regarding each query.\n",
    "\n",
    "This array stores the starting offset of each query (starting from the first instance identified by the first row). Its shape is thus (n_queries+1), with the latter element that is not strictly needed but is put there for simplicity (its value is always the number of instances in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6001,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.query_offsets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this information, the i-th query has indices ranging from query_offsets[i] up to query_offsets[i+1], with the latter element excluded. For example, the first query start at index 0 and contains 86 documents up to index 86 (excluded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 86])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.query_offsets[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of documents in each query can thus be obtained automatically from the `query_offset` variable. However, RankEval provide a simple utility for accessing this information, that is the `get_query_sizes` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 86, 106,  92, ...,  79, 180,  40])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.get_query_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns a vector (numpy 1d array) with a single element for each query, identifying the number of documents belonging to this query. As noticed above, the first query has 86 documents, the second has 106 documents, and so on and so forth. The shape of this vector is thus (n_queries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.get_query_sizes().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situation we need to have the list of query ids, one for each instance (in place of the compact version adopted by RankEval). For example, this feature is needed by LightGBM when you need to create a dataset. RankEval provides such information with the `get_qids_dataset` utility, returning a vector (numpy 1d array) with shape (n_instances):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723412,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.get_qids_dataset().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first document has qid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msn_train.get_qids_dataset()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over each query of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime it is needed to iterate over the low level information stored in the dataset instance. RankEval allows you to do that with the `query_iterator` utility, that provides you with an iterator over the offsets of the query_ids in the dataset. In particular, each element of the iterator is a tuple (qid, start_offset, end_offset), highlighting thus the qid of the query and the row index of the instances belonging it. For example, the first element of the iterator is related to the query with `qid=1` and offset ranging from 0 to 86 (excluded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 86)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(msn_train.query_iterator())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
