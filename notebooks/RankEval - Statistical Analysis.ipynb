{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "\n",
    "**RankEval** provides the following statistical analysis tools: *i)* Fisher's randomization test for statistical significance, and *ii)* bias/variance decomposition of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import common libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance\n",
    "\n",
    "According to the work by *M.D. Smucker, J. Allan, B. Carterette, \"A Comparison of Statistical Significance Tests for Information Retrieval Evaluation\", CIKM 2007*, **Fisher's randomization test** is the most appropriate statistical test to evaluate wheter two rankers differ significantly.\n",
    "\n",
    "We first shortly describe the test. The null hypthesis is that the two given rankers A and B are indentical: an underlying ranker R is asked to produce two rankings for each given query  and these two rankings are randomly labeled as ranker A or ranker B. The goal of the test is to measure the probability that the observed performance gap between ranker A and B is due to a random labeling.\n",
    "\n",
    "Under the null hypthesis, every permutation of the labelling is equally probable. If we enumerate all the possible A-B labelings, and we measure the corresponding quality gap, we have that:\n",
    " - the *one-sided p-value* is given by the fraction of times the quality difference is larger than the originally observed difference;\n",
    " - the *two-sided p-value* is given by the fraction of times the resulting quality *absolute difference* is larger than the originally observed difference.\n",
    "\n",
    "Since the number of permutations is exponential in the number of queries, a large number of random permutations is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import RankEval statistical significance tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.model import RTEnsemble\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.metrics import NDCG\n",
    "from rankeval.metrics import Precision\n",
    "from rankeval.analysis.statistical import statistical_significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load models and data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "dataset_file = \"/home/rankeval/rankeval_data/msn/dataset/Fold1/test.txt\"\n",
    "\n",
    "qr_1K_file  = \"/home/rankeval/rankeval_data/msn/models/Fold1/msn1.quickrank.LAMBDAMART.20000.32.T1000.xml\"\n",
    "qr_10K_file = \"/home/rankeval/rankeval_data/msn/models/Fold1/msn1.quickrank.LAMBDAMART.20000.32.T10000.xml\"\n",
    "lgbm_1K_file   = \"/home/rankeval/rankeval_data/msn/models/Fold1/msn1.lightgbm.LAMBDAMART.1000.32.T1000.model\"\n",
    "\n",
    "# load\n",
    "qr_1K   = RTEnsemble(lmart_1K_file, name=\"QuickRank.1k\", format=\"QuickRank\")\n",
    "qr_10K  = RTEnsemble(lmart_10K_file, name=\"QuickRank.10k\", format=\"QuickRank\")\n",
    "lgbm_1K = RTEnsemble(lgbm_1K_file, name=\"LGBM.1k\", format=\"LightGBM\")\n",
    "\n",
    "msn1 = Dataset.load(dataset_file, name=\"Msn - Fold 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the Fisher's Randomization test\n",
    "\n",
    "The `statistical_significance` test between a two rankers can be run on a list of datasets and for a list of IR quality metrics. The function returns both the one-sided and two-sided p-values.\n",
    "\n",
    "We first compare the three models we loaded above. We can observe below that the QuickRank model with 10k trees performs worse that the QuickRank 1k tree: this is due to the overfitting of such a large model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Model Performance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Msn - Fold 1</th>\n",
       "      <th>QuickRank.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.529570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuickRank.10k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.510248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.524908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model Performance\n",
       "dataset      model         metric                    \n",
       "Msn - Fold 1 QuickRank.1k  NDCG@10           0.529570\n",
       "             QuickRank.10k NDCG@10           0.510248\n",
       "             LGBM.1k       NDCG@10           0.524908"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rankeval.analysis.effectiveness import model_performance\n",
    "\n",
    "ndcg_10 = NDCG(cutoff=10)\n",
    "\n",
    "perf = model_performance(datasets=[msn1], \n",
    "                         models=[qr_1K, qr_10K, lgbm_1K], \n",
    "                         metrics=[ndcg_10])\n",
    "perf.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also observe that the QuickRank.1k model performs better than LGBM.1k with only a small difference. We therefore measure whether this difference is statistically significant as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Statistical Significance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>metric</th>\n",
       "      <th>p-value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Msn - Fold 1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">NDCG@10</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.02084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.04202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Statistical Significance\n",
       "dataset      metric  p-value                            \n",
       "Msn - Fold 1 NDCG@10 one-sided                   0.02084\n",
       "                     two-sided                   0.04202"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_sig = statistical_significance(datasets=[msn1],\n",
    "                                    model_a=qr_1K, model_b=lgbm_1K, \n",
    "                                    metrics=[ndcg_10],\n",
    "                                    n_perm=100000 )\n",
    "stat_sig.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We conclude that the difference is statistically significant at $p<0.05$. To conclude the analysis, we evaluate the performance of the two algorithms also with NDCG@50 and Precision@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Model Performance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">Msn - Fold 1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">QuickRank.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.529570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@50</th>\n",
       "      <td>0.605428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision@10[&gt;=1]</th>\n",
       "      <td>0.657644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">LGBM.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.524908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@50</th>\n",
       "      <td>0.600480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision@10[&gt;=1]</th>\n",
       "      <td>0.655794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model Performance\n",
       "dataset      model        metric                              \n",
       "Msn - Fold 1 QuickRank.1k NDCG@10                     0.529570\n",
       "                          NDCG@50                     0.605428\n",
       "                          Precision@10[>=1]           0.657644\n",
       "             LGBM.1k      NDCG@10                     0.524908\n",
       "                          NDCG@50                     0.600480\n",
       "                          Precision@10[>=1]           0.655794"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_50 = NDCG(cutoff=50)\n",
    "prec_10 = Precision(cutoff=10)\n",
    "\n",
    "perf = model_performance(datasets=[msn1], \n",
    "                         models=[qr_1K, lgbm_1K], \n",
    "                         metrics=[ndcg_10, ndcg_50, prec_10])\n",
    "perf.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Statistical Significance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>metric</th>\n",
       "      <th>p-value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">Msn - Fold 1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">NDCG@10</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.02094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.04120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">NDCG@50</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.00012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.00023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Precision@10[&gt;=1]</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.23902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.47691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Statistical Significance\n",
       "dataset      metric            p-value                            \n",
       "Msn - Fold 1 NDCG@10           one-sided                   0.02094\n",
       "                               two-sided                   0.04120\n",
       "             NDCG@50           one-sided                   0.00012\n",
       "                               two-sided                   0.00023\n",
       "             Precision@10[>=1] one-sided                   0.23902\n",
       "                               two-sided                   0.47691"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_sig = statistical_significance(datasets=[msn1],\n",
    "                                    model_a=qr_1K, model_b=lgbm_1K, \n",
    "                                    metrics=[ndcg_10, ndcg_50, prec_10],\n",
    "                                    n_perm=100000 )\n",
    "stat_sig.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance\n",
    "\n",
    "The Error of a Algorithm can be decomposed in:\n",
    "$$E(A) = Bias(A) + Variance(A) + Noise(A)$$\n",
    "where:\n",
    " - Bias is how far is the model from the prediction\n",
    " - Variance is how sensitive (how changes) the prediction with different training sets (overfitting)\n",
    " - Noise is the irreducible error in the dataset (learner independent)\n",
    "\n",
    "**RankEval** supports the computation of the bias vs. variance decomposition of the error.\n",
    "The approach used is based on the works of [Webb05] and [Dom05]. As in other works, we hereinafter assume noise is absent.\n",
    "\n",
    "RankEval allows to decompose the errore according to a given user provided (IR) quality metric as follows.\n",
    "\n",
    "Each instance of the dataset is scored *L* times.\n",
    "A single scoring is achieved by splitting the dataset at random into\n",
    "*k* folds. Each fold is scored by the model *M* trained with the algorithm $A$ on the remainder folds.\n",
    "[Webb05] recommends the use of 2 folds.\n",
    "\n",
    "If the metric used is Mean Squared Error then the standard decomposition is used.\n",
    "The Bias for and instance *x* is defined as mean squared error of the *L* trained models\n",
    "w.r.t. the true label *y*, denoted with ${\\sf E}_{L} [M(x) - y]^2$. \n",
    "The Variance for an instance *x* is measured across the *L* trained models: \n",
    "${\\sf E}_{L} [M(x) - {\\sf E}_{L} M(x)]^2$. \n",
    "Both are averaged over all instances in the dataset.\n",
    "\n",
    "If the metric is any of the IR quality measures, we resort to the bias variance\n",
    "decomposition of the mean squared error of the given metric w.r.t. its ideal value,\n",
    "e.g., for the case of NDCG, ${\\sf E}_{L} [1 - {\\sf NDCG}]^2$. \n",
    "Recall that, a formal Bias/Variance decomposition was not proposed yet.\n",
    "\n",
    "##### References\n",
    " - [Webb05] Webb, Geoffrey I., and Paul Conilione. \"Estimating bias and variance from data.\" Pre-publication manuscript (2005).\n",
    " - [Dom05] Domingos P. A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning 2000 (pp. 231-238)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset and define metrics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.analysis.statistical import bias_variance\n",
    "\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.metrics import NDCG\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "msn1 = Dataset.load(\"/home/rankeval/rankeval_data/msn/dataset/Fold1/test.txt\", name=\"MSN - Fold 1\")\n",
    "\n",
    "ndcg_10 = NDCG(cutoff=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the algorithm of wich we want to measure its bias/variance decomposition\n",
    "\n",
    "The Bias/Variancs decomposition is a measure of a given algorithm with given parameters. Recall that RankEval needs to repeatedly train and evaluate models learnt by the given algorithm. To do so, we define a wrapper function to be used by RankEval with the following parameters:\n",
    " - `train_X`: numpy.ndarray storing a 2-D matrix of size num_docs x num_features\n",
    " - `train_Y`: numpy.ndarray storing a vector of document's relevance labels\n",
    " - `train_q`: numpy.ndarray storing a vector of query lengths\n",
    " - `test_X`: numpy.ndarray as for `train_X`\n",
    "Such wrapper function trains a new model on `train_X`, `train_Y`, `train_q`, then used to score `test_X`.\n",
    "An `numpy.ndarray` with such scores is returned.\n",
    "\n",
    "In the example below we use LightGBM, for which we define a two wrapper function for training forests of 100 trees and with eithr 32 (`lgbm_small_wrapper`) or 64 (`lgbm_large_wrapper`) leaves each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "def lgbm_algo(trees, leaves, train_X, train_Y, train_q, test_X):\n",
    "    params = {'num_leaves': leaves, 'objective':'lambdarank',\n",
    "             'learning_rate': 0.01, 'max_bin': 1024}\n",
    "\n",
    "    training = lightgbm.Dataset(data=train_X, label=train_Y, group=train_q)\n",
    "    \n",
    "    bst = lightgbm.train(params, training, num_boost_round=trees)\n",
    "    \n",
    "    return bst.predict(test_X)\n",
    "\n",
    "def lgbm_small_wrapper(train_X, train_Y, train_q, test_X):\n",
    "    return lgbm_algo(100, 16, train_X, train_Y, train_q, test_X)\n",
    "\n",
    "def lgbm_large_wrapper(train_X, train_Y, train_q, test_X):\n",
    "    return lgbm_algo(100, 128, train_X, train_Y, train_q, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the bias/variance decomposition\n",
    "\n",
    "The function `bias_variance` returns a 3-tuple with: \n",
    " - the average loss according to the given metric\n",
    " - the average bias\n",
    " - the average variance\n",
    "\n",
    "Below the bias variance decomposition for the MSE and for NDCG@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b658b73912d646e58c5453723375e0d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "bias_variance() got an unexpected keyword argument 'progress_bar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-34099162d280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msmall_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgbm_small_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Small model - MSE decomposition:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Error   :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_mse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bias_variance() got an unexpected keyword argument 'progress_bar'"
     ]
    }
   ],
   "source": [
    "progress_bar = FloatProgress(min=0, max=100)\n",
    "display(progress_bar)\n",
    "\n",
    "small_mse = bias_variance(msn1, algo=lgbm_small_wrapper, metric=\"mse\", L=5, k=2, progress_bar=progress_bar)\n",
    "print \"Small model - MSE decomposition:\"\n",
    "print \"Error   :\", small_mse[0]\n",
    "print \"Bias    :\", small_mse[1]\n",
    "print \"Variace :\", small_mse[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " + Dataset scoring 0 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 1 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 2 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 3 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 4 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      "Large model - MSE decomposition:\n",
      "Error   : 1.22656\n",
      "Bias    : 1.22546\n",
      "Variace : 0.00109688\n"
     ]
    }
   ],
   "source": [
    "large_mse = bias_variance(msn1, algo=lgbm_large_wrapper, metric=\"mse\", L=5, k=2, verbose=2)\n",
    "print \"Large model - MSE decomposition:\"\n",
    "print \"Error   :\", large_mse[0]\n",
    "print \"Bias    :\", large_mse[1]\n",
    "print \"Variace :\", large_mse[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " + Dataset scoring 0 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 1 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 2 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 3 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 4 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.32475391, 0.32101443, 0.0037394969)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ndcg = bias_variance(msn1, algo=lgbm_small_wrapper, metric=ndcg_10, L=5, k=2, verbose=2)\n",
    "print \"Small model - NDCG decomposition:\"\n",
    "print \"Error   :\", small_ndcg[0]\n",
    "print \"Bias    :\", small_ndcg[1]\n",
    "print \"Variace :\", small_ndcg[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " + Dataset scoring 0 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 1 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 2 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 3 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n",
      " + Dataset scoring 4 of 5\n",
      "   - Processing fold 0 of 2\n",
      "   - Processing fold 1 of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.31129625, 0.30602935, 0.0052669076)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_ndcg = bias_variance(msn1, algo=lgbm_large_wrapper, metric=ndcg_10, L=5, k=2, verbose=2)\n",
    "print \"Large model - NDCG decomposition:\"\n",
    "print \"Error   :\", large_ndcg[0]\n",
    "print \"Bias    :\", large_ndcg[1]\n",
    "print \"Variace :\", large_ndcg[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
