{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "\n",
    "**RankEval** provides the following statistical analysis tools: *i)* Fisher's randomization test for statistical significance, and *ii)* bias/variance decomposition of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import common libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance\n",
    "\n",
    "According to the work by *M.D. Smucker, J. Allan, B. Carterette, \"A Comparison of Statistical Significance Tests for Information Retrieval Evaluation\", CIKM 2007*, **Fisher's randomization test** is the most appropriate statistical test to evaluate wheter two rankers differ significantly.\n",
    "\n",
    "We first shortly describe the test. The null hypthesis is that the two given rankers A and B are indentical: an underlying ranker R is asked to produce two rankings for each given query  and these two rankings are randomly labeled as ranker A or ranker B. The goal of the test is to measure the probability that the observed performance gap between ranker A and B is due to a random labeling.\n",
    "\n",
    "Under the null hypthesis, every permutation of the labelling is equally probable. If we enumerate all the possible A-B labelings, and we measure the corresponding quality gap, we have that:\n",
    " - the *one-sided p-value* is given by the fraction of times the quality difference is larger than the originally observed difference;\n",
    " - the *two-sided p-value* is given by the fraction of times the resulting quality *absolute difference* is larger than the originally observed difference.\n",
    "\n",
    "Since the number of permutations is exponential in the number of queries, a large number of random permutations is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import RankEval statistical significance tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.model import RTEnsemble\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.dataset.datasets_fetcher import load_dataset\n",
    "from rankeval.metrics import NDCG\n",
    "from rankeval.metrics import Precision\n",
    "from rankeval.analysis.statistical import statistical_significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load models and data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files. This may take a few minutes.\n",
      "done loading dataset!\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset_container = load_dataset(dataset_name='msn10k', \n",
    "                                fold='1', \n",
    "                                download_if_missing=True, \n",
    "                                force_download=False, \n",
    "                                with_models=True)\n",
    "\n",
    "# we use the test dataset here\n",
    "msn1 = dataset_container.test_dataset\n",
    "msn1.name = \"MSN10K - Fold 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /home/rankeval/rankeval_data/msn10k/models/Fold1/lightgbm/LGBM.msn10k.fold-1.lambdarank.leaves-32.lr-5.trees-1000.model\n",
      "1 /home/rankeval/rankeval_data/msn10k/models/Fold1/catboost/msn1.catboost.LAMBDAMART.1000.5.T1000.json\n",
      "2 /home/rankeval/rankeval_data/msn10k/models/Fold1/catboost/msn1.catboost.LAMBDAMART.1000.5.T1000.model\n",
      "3 /home/rankeval/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T20000.xml\n",
      "4 /home/rankeval/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T5000.xml\n",
      "5 /home/rankeval/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T1000.xml\n",
      "6 /home/rankeval/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T15000.xml\n",
      "7 /home/rankeval/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T10000.xml\n",
      "8 /home/rankeval/rankeval_data/msn10k/models/Fold1/xgboost/XGBOOST.msn10k.fold-1.pairwise.d-5.lr-10.trees-1000.model\n",
      "9 /home/rankeval/rankeval_data/msn10k/models/Fold4/lightgbm/LGBM.msn10k.fold-4.lambdarank.leaves-32.lr-5.trees-1000.model\n",
      "10 /home/rankeval/rankeval_data/msn10k/models/Fold4/xgboost/XGBOOST.msn10k.fold-4.pairwise.d-5.lr-10.trees-1000.model\n",
      "11 /home/rankeval/rankeval_data/msn10k/models/Fold5/lightgbm/LGBM.msn10k.fold-5.lambdarank.leaves-32.lr-5.trees-1000.model\n",
      "12 /home/rankeval/rankeval_data/msn10k/models/Fold5/xgboost/XGBOOST.msn10k.fold-5.pairwise.d-5.lr-10.trees-1000.model\n",
      "13 /home/rankeval/rankeval_data/msn10k/models/Fold2/lightgbm/LGBM.msn10k.fold-2.lambdarank.leaves-32.lr-5.trees-1000.model\n",
      "14 /home/rankeval/rankeval_data/msn10k/models/Fold2/xgboost/XGBOOST.msn10k.fold-2.pairwise.d-5.lr-10.trees-1000.model\n",
      "15 /home/rankeval/rankeval_data/msn10k/models/Fold3/lightgbm/LGBM.msn10k.fold-3.lambdarank.leaves-32.lr-5.trees-1000.model\n",
      "16 /home/rankeval/rankeval_data/msn10k/models/Fold3/xgboost/XGBOOST.msn10k.fold-3.pairwise.d-5.lr-10.trees-1000.model\n"
     ]
    }
   ],
   "source": [
    "# view available models\n",
    "for item, file_name in enumerate(dataset_container.model_filenames):\n",
    "    print item, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model files\n",
    "qr_10K_file  = dataset_container.model_filenames[7] # \"msn1.quickrank.LAMBDAMART.20000.32.T10000.xml\"\n",
    "qr_1K_file   = dataset_container.model_filenames[5] # \"msn1.quickrank.LAMBDAMART.20000.32.T1000.xml\"\n",
    "lgbm_1K_file = dataset_container.model_filenames[0] # \"LGBM.msn10k.fold-1.lambdarank.leaves-32.lr-5.trees-1000.model\"\n",
    "xbg_1k_file  = dataset_container.model_filenames[8] # \"XGBOOST.msn10k.fold-1.pairwise.d-5.lr-10.trees-1000.model\"\n",
    "\n",
    "qr_1K   = RTEnsemble(qr_1K_file, name=\"QuickRank.1k\", format=\"QuickRank\")\n",
    "qr_10K  = RTEnsemble(qr_10K_file, name=\"QuickRank.10k\", format=\"QuickRank\")\n",
    "lgbm_1K = RTEnsemble(lgbm_1K_file, name=\"LGBM.1k\", format=\"LightGBM\")\n",
    "xgb_1K = RTEnsemble(xbg_1k_file, name=\"XGB.1k\", format=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the Fisher's Randomization test\n",
    "\n",
    "The `statistical_significance` test between a two rankers can be run on a list of datasets and for a list of IR quality metrics. The function returns both the one-sided and two-sided p-values.\n",
    "\n",
    "We first compare the three models we loaded above. We can observe below that the QuickRank model with 10k trees performs worse that the QuickRank with 1k tree: this is due to the overfitting of such a large model. Also, LightGBM is the worst performing alorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Model Performance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MSN10K - Fold 1</th>\n",
       "      <th>QuickRank.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.492246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuickRank.10k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.474035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.507996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.487309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Model Performance\n",
       "dataset         model         metric                    \n",
       "MSN10K - Fold 1 QuickRank.1k  NDCG@10           0.492246\n",
       "                QuickRank.10k NDCG@10           0.474035\n",
       "                LGBM.1k       NDCG@10           0.507996\n",
       "                XGB.1k        NDCG@10           0.487309"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rankeval.analysis.effectiveness import model_performance\n",
    "\n",
    "ndcg_10 = NDCG(cutoff=10)\n",
    "\n",
    "perf = model_performance(datasets=[msn1], \n",
    "                         models=[qr_1K, qr_10K, lgbm_1K, xgb_1K], \n",
    "                         metrics=[ndcg_10])\n",
    "perf.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also observe that the QuickRank.1k model performs better than XGBoost.1k with only a small difference. We therefore measure whether this difference is statistically significant as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a534c59b7148eeb5c98d0359d571e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a274d353384ad7afaf8e42c5e0206d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Statistical Significance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>metric</th>\n",
       "      <th>p-value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MSN10K - Fold 1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">NDCG@10</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.03412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.06830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Statistical Significance\n",
       "dataset         metric  p-value                            \n",
       "MSN10K - Fold 1 NDCG@10 one-sided                   0.03412\n",
       "                        two-sided                   0.06830"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_sig = statistical_significance(datasets=[msn1],\n",
    "                                    model_a=qr_1K, model_b=xgb_1K, \n",
    "                                    metrics=[ndcg_10],\n",
    "                                    n_perm=100000 )\n",
    "stat_sig.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We conclude that the difference is statistically significant at $p<0.05$ only for the one-sided test. To conclude the analysis, we evaluate the performance of the two algorithms also with NDCG@50 and Precision@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Model Performance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MSN10K - Fold 1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">QuickRank.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.492246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@50</th>\n",
       "      <td>0.597562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@10</th>\n",
       "      <td>0.657644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">XGB.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.487309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@50</th>\n",
       "      <td>0.595671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@10</th>\n",
       "      <td>0.682194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Model Performance\n",
       "dataset         model        metric                    \n",
       "MSN10K - Fold 1 QuickRank.1k NDCG@10           0.492246\n",
       "                             NDCG@50           0.597562\n",
       "                             P@10              0.657644\n",
       "                XGB.1k       NDCG@10           0.487309\n",
       "                             NDCG@50           0.595671\n",
       "                             P@10              0.682194"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_50 = NDCG(cutoff=50)\n",
    "prec_10 = Precision(cutoff=10)\n",
    "\n",
    "perf = model_performance(datasets=[msn1], \n",
    "                         models=[qr_1K, xgb_1K], \n",
    "                         metrics=[ndcg_10, ndcg_50, prec_10])\n",
    "perf.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cad63d0463940b18aa1a8261d02c47e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee6f45a084c41158bd9dd6bfc68e513"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64cf06d7068481face9d9425d70671f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f179a8095f249a3bd8d62386586a26f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Statistical Significance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>metric</th>\n",
       "      <th>p-value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MSN10K - Fold 1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">NDCG@10</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.03474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.06961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">NDCG@50</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.14742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.29608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">P@10</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Statistical Significance\n",
       "dataset         metric  p-value                            \n",
       "MSN10K - Fold 1 NDCG@10 one-sided                   0.03474\n",
       "                        two-sided                   0.06961\n",
       "                NDCG@50 one-sided                   0.14742\n",
       "                        two-sided                   0.29608\n",
       "                P@10    one-sided                   0.00000\n",
       "                        two-sided                   0.00000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_sig = statistical_significance(datasets=[msn1],\n",
    "                                    model_a=qr_1K, model_b=xgb_1K, \n",
    "                                    metrics=[ndcg_10, ndcg_50, prec_10],\n",
    "                                    n_perm=100000 )\n",
    "stat_sig.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above experiments, we see no sigificant differences in terms of NDCG@50, but the preformance of XGBoost is better and statistically significant in terms of Precision@10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias vs. Variance decomposition\n",
    "\n",
    "The Error of a Algorithm can be decomposed in:\n",
    "$$E(A) = Bias(A) + Variance(A) + Noise(A)$$\n",
    "where:\n",
    " - Bias is how far is the model from the prediction\n",
    " - Variance is how sensitive (how changes) the prediction with different training sets (overfitting)\n",
    " - Noise is the irreducible error in the dataset (learner independent)\n",
    "\n",
    "**RankEval** supports the computation of the bias vs. variance decomposition of the error.\n",
    "The approach used is based on the works of [Webb05] and [Dom05]. As in other works, we hereinafter assume noise is absent.\n",
    "\n",
    "RankEval allows to decompose the errore according to a given user provided (IR) quality metric as follows.\n",
    "\n",
    "Each instance of the dataset is scored *L* times.\n",
    "A single scoring is achieved by splitting the dataset at random into\n",
    "*k* folds. Each fold is scored by the model *M* trained with the algorithm $A$ on the remainder folds.\n",
    "[Webb05] recommends the use of 2 folds.\n",
    "\n",
    "If the metric used is Mean Squared Error then the standard decomposition is used.\n",
    "The Bias for and instance *x* is defined as mean squared error of the *L* trained models\n",
    "w.r.t. the true label *y*, denoted with ${\\sf E}_{L} [M(x) - y]^2$. \n",
    "The Variance for an instance *x* is measured across the *L* trained models: \n",
    "${\\sf E}_{L} [M(x) - {\\sf E}_{L} M(x)]^2$. \n",
    "Both are averaged over all instances in the dataset.\n",
    "\n",
    "If the metric is any of the IR quality measures, we resort to the bias variance\n",
    "decomposition of the mean squared error of the given metric w.r.t. its ideal value,\n",
    "e.g., for the case of NDCG, ${\\sf E}_{L} [1 - {\\sf NDCG}]^2$. \n",
    "Recall that, a formal Bias/Variance decomposition was not proposed yet.\n",
    "\n",
    "##### References\n",
    " - [Webb05] Webb, Geoffrey I., and Paul Conilione. \"Estimating bias and variance from data.\" Pre-publication manuscript (2005).\n",
    " - [Dom05] Domingos P. A. \"Unified bias-variance decomposition.\" In Proceedings of 17th International Conference on Machine Learning 2000 (pp. 231-238)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset and define metrics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.analysis.statistical import bias_variance\n",
    "\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.dataset.datasets_fetcher import load_dataset\n",
    "from rankeval.metrics import NDCG\n",
    "from rankeval.metrics import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files. This may take a few minutes.\n",
      "done loading dataset!\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset_container = load_dataset(dataset_name='msn10k', \n",
    "                                fold='1', \n",
    "                                download_if_missing=True, \n",
    "                                force_download=False, \n",
    "                                with_models=True)\n",
    "\n",
    "# we use the test dataset here\n",
    "msn1 = dataset_container.test_dataset\n",
    "msn1.name = \"MSN10K - Fold 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the algorithm of wich we want to measure its bias/variance decomposition\n",
    "\n",
    "The Bias/Variancs decomposition is a measure of a given algorithm with given parameters. Recall that RankEval needs to repeatedly train and evaluate models learnt by the given algorithm. To do so, we define a wrapper function to be used by RankEval with the following parameters:\n",
    " - `train_X`: numpy.ndarray storing a 2-D matrix of size num_docs x num_features\n",
    " - `train_Y`: numpy.ndarray storing a vector of document's relevance labels\n",
    " - `train_q`: numpy.ndarray storing a vector of query lengths\n",
    " - `test_X`: numpy.ndarray as for `train_X`\n",
    "Such wrapper function trains a new model on `train_X`, `train_Y`, `train_q`, then used to score `test_X`.\n",
    "An `numpy.ndarray` with such scores is returned.\n",
    "\n",
    "In the example below we use LightGBM, for which we define a two wrapper function for training forests of 100 trees and with eithr 32 (`lgbm_small_wrapper`) or 64 (`lgbm_large_wrapper`) leaves each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "def lgbm_algo(trees, leaves, train_X, train_Y, train_q, test_X):\n",
    "    params = {'num_leaves': leaves, 'objective':'lambdarank', 'learning_rate': 0.05}\n",
    "\n",
    "    training = lightgbm.Dataset(data=train_X, label=train_Y, group=train_q)\n",
    "    \n",
    "    bst = lightgbm.train(params, training, num_boost_round=trees)\n",
    "    \n",
    "    return bst.predict(test_X)\n",
    "\n",
    "def lgbm_small(train_X, train_Y, train_q, test_X):\n",
    "    return lgbm_algo(100, 16, train_X, train_Y, train_q, test_X)\n",
    "\n",
    "def lgbm_large(train_X, train_Y, train_q, test_X):\n",
    "    return lgbm_algo(100, 128, train_X, train_Y, train_q, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the bias/variance decomposition\n",
    "\n",
    "The function `bias_variance` returns an `array.DataArray` containing the bias/variance decomposition of the error for the given datasets, algorithms and metrics.\n",
    "\n",
    "Below the bias variance decomposition for the two LightGBM models for both the MSE and NDCG@10 losses. \n",
    "\n",
    "The error is dominated by the Bias component. Interestingly, the variance increases by a factor of 2 with the larger model including 128 leaves. Also note that the smaller model is the best performing in terms of MSE but not in terms of NDCG. This is because the model was trained by optimizing the Lambda-Rank loss and not the MSE: predicted scores may deviate from the target labels, but the resulting ranking is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a0257ed547400f87dc30f63115afb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c8b44f1924495ca5d2afeadf71d32a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75a07d1f8fa43db91c605ea5a6e3f0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06420a3214c4761b7dfe916ab9c1c37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33376b6f5ec44e63a37130296c94913b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3832f6659dfc445b8e5aed8290121f5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f90b3fb82949b59000100ad8efebe7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7801d8aedc2c4140ad7c9644565c4846"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6538e99aba4336a2f18319fd1798fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41c633587e44b70a44dc82c48a56781"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1d4e9a4419434fab2c8f9620df7b6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2303eded6e469bb07f0d73b49076f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430df718d8aa4763bcf2bde03c0ea235"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a40b08292f54d5392e0b77cfc45cc84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dd5f2e2cbf4780aca5f7217fdc9d2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf651bb7dd64fe2b86b60dcaa3ed38d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b243f052894c4e329d56fe4254d58f4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5065a5acc76344d3b7022bf108af63d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4122ca919c3b4dc794c036599c5339e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cff95b9acc5449f92407db215034773"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e545fe87a045a89c35d6dcd4e34d50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae7d3ed04ef43c6a88e2475590fee0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b9c1142ec24824bb1a2cf7936c7ea6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5142e962ee144c5964a359de4f045fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1744fe8cac4749b7b2f4efd4a6d7de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Bias/Variance Decomposition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>metric</th>\n",
       "      <th>algo</th>\n",
       "      <th>error</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">MSN10K - Fold 1</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">MSE</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">lgbm_small</th>\n",
       "      <th>Error</th>\n",
       "      <td>1.286307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>1.284281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.002025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">lgbm_large</th>\n",
       "      <th>Error</th>\n",
       "      <td>1.421161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>1.416336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.004825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">NDCG@10</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">lgbm_small</th>\n",
       "      <th>Error</th>\n",
       "      <td>0.344878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>0.341951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.002927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">lgbm_large</th>\n",
       "      <th>Error</th>\n",
       "      <td>0.329454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>0.324732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.004721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Bias/Variance Decomposition\n",
       "dataset         metric  algo       error                                \n",
       "MSN10K - Fold 1 MSE     lgbm_small Error                        1.286307\n",
       "                                   Bias                         1.284281\n",
       "                                   Variance                     0.002025\n",
       "                        lgbm_large Error                        1.421161\n",
       "                                   Bias                         1.416336\n",
       "                                   Variance                     0.004825\n",
       "                NDCG@10 lgbm_small Error                        0.344878\n",
       "                                   Bias                         0.341951\n",
       "                                   Variance                     0.002927\n",
       "                        lgbm_large Error                        0.329454\n",
       "                                   Bias                         0.324732\n",
       "                                   Variance                     0.004721"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bv = bias_variance(datasets=[msn1], \n",
    "                   algos=[lgbm_small, lgbm_large], \n",
    "                   metrics=[MSE(), NDCG(cutoff=10)], \n",
    "                   L=5, k=2)\n",
    "bv.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
