{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "\n",
    "**RankEval** provides the following statistical analysis tools: *i)* Fisher's randomization test for statistical significance, and *ii)* bias/variance decomposition of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of rankeval.analysis.statistical failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name MSE\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# import common libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance\n",
    "\n",
    "According to the work by *M.D. Smucker, J. Allan, B. Carterette, \"A Comparison of Statistical Significance Tests for Information Retrieval Evaluation\", CIKM 2007*, **Fisher's randomization test** is the most appropriate statistical test to evaluate wheter two rankers differ significantly.\n",
    "\n",
    "We first shortly describe the test. The null hypthesis is that the two given rankers A and B are indentical: an underlying ranker R is asked to produce two rankings for each given query  and these two rankings are randomly labeled as ranker A or ranker B. The goal of the test is to measure the probability that the observed performance gap between ranker A and B is due to a random labeling.\n",
    "\n",
    "Under the null hypthesis, every permutation of the labelling is equally probable. If we enumerate all the possible A-B labelings, and we measure the corresponding quality gap, we have that:\n",
    " - the *one-sided p-value* is given by the fraction of times the quality difference is larger than the originally observed difference;\n",
    " - the *two-sided p-value* is given by the fraction of times the resulting quality *absolute difference* is larger than the originally observed difference.\n",
    "\n",
    "Since the number of permutations is exponential in the number of queries, a large number of random permutations is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import RankEval statistical significance tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rankeval.model import RTEnsemble\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.dataset.datasets_fetcher import load_dataset\n",
    "from rankeval.metrics import NDCG\n",
    "from rankeval.metrics import Precision\n",
    "from rankeval.analysis.statistical import statistical_significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load models and data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files. This may take a few minutes.\n",
      "done loading dataset!\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "qr_1K_file  = \"/home/rankeval/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T1000.xml\"\n",
    "qr_10K_file = \"/home/rankeval/rankeval_data/msn10k/models/Fold1/quickrank/msn1.quickrank.LAMBDAMART.20000.32.T10000.xml\"\n",
    "lgbm_1K_file= \"./LGBM.msn10k.fold-1.lambdarank.leaves-32.lr-5.trees-1000.model\"\n",
    "xbg_1k_file = \"./XGBOOST.msn10k.fold-1.pairwise.d-5.lr-10.trees-1000.model\"\n",
    "\n",
    "qr_1K   = RTEnsemble(qr_1K_file, name=\"QuickRank.1k\", format=\"QuickRank\")\n",
    "qr_10K  = RTEnsemble(qr_10K_file, name=\"QuickRank.10k\", format=\"QuickRank\")\n",
    "lgbm_1K = RTEnsemble(lgbm_1K_file, name=\"LGBM.1k\", format=\"LightGBM\")\n",
    "xgb_1K = RTEnsemble(xbg_1k_file, name=\"XGB.1k\", format=\"XGBoost\")\n",
    "\n",
    "# load data\n",
    "dataset_container = load_dataset(dataset_name='msn10k', \n",
    "                                fold='1', \n",
    "                                download_if_missing=True, \n",
    "                                force_download=False, \n",
    "                                with_models=False)\n",
    "\n",
    "# we use the test dataset here\n",
    "msn1 = dataset_container.test_dataset\n",
    "msn1.name = \"MSN10K - Fold 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the Fisher's Randomization test\n",
    "\n",
    "The `statistical_significance` test between a two rankers can be run on a list of datasets and for a list of IR quality metrics. The function returns both the one-sided and two-sided p-values.\n",
    "\n",
    "We first compare the three models we loaded above. We can observe below that the QuickRank model with 10k trees performs worse that the QuickRank with 1k tree: this is due to the overfitting of such a large model. Also, LightGBM is the worst performing alorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Model Performance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MSN10K - Fold 1</th>\n",
       "      <th>QuickRank.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.492246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuickRank.10k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.474035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.467710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.487220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Model Performance\n",
       "dataset         model         metric                    \n",
       "MSN10K - Fold 1 QuickRank.1k  NDCG@10           0.492246\n",
       "                QuickRank.10k NDCG@10           0.474035\n",
       "                LGBM.1k       NDCG@10           0.467710\n",
       "                XGB.1k        NDCG@10           0.487220"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rankeval.analysis.effectiveness import model_performance\n",
    "\n",
    "ndcg_10 = NDCG(cutoff=10)\n",
    "\n",
    "perf = model_performance(datasets=[msn1], \n",
    "                         models=[qr_1K, qr_10K, lgbm_1K, xgb_1K], \n",
    "                         metrics=[ndcg_10])\n",
    "perf.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also observe that the QuickRank.1k model performs better than XGBoost.1k with only a small difference. We therefore measure whether this difference is statistically significant as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e89b18f2ec49e6a59767a595264162"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352208b6017248e497695630ba70068e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Statistical Significance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>metric</th>\n",
       "      <th>p-value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MSN10K - Fold 1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">NDCG@10</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.03265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.06482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Statistical Significance\n",
       "dataset         metric  p-value                            \n",
       "MSN10K - Fold 1 NDCG@10 one-sided                   0.03265\n",
       "                        two-sided                   0.06482"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_sig = statistical_significance(datasets=[msn1],\n",
    "                                    model_a=qr_1K, model_b=xgb_1K, \n",
    "                                    metrics=[ndcg_10],\n",
    "                                    n_perm=100000 )\n",
    "stat_sig.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We conclude that the difference is statistically significant at $p<0.05$ only for the one-sided test. To conclude the analysis, we evaluate the performance of the two algorithms also with NDCG@50 and Precision@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Model Performance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MSN10K - Fold 1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">QuickRank.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.492246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@50</th>\n",
       "      <td>0.597562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@10[&gt;=1]</th>\n",
       "      <td>0.657644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">XGB.1k</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.487220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@50</th>\n",
       "      <td>0.595683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@10[&gt;=1]</th>\n",
       "      <td>0.682194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Model Performance\n",
       "dataset         model        metric                      \n",
       "MSN10K - Fold 1 QuickRank.1k NDCG@10             0.492246\n",
       "                             NDCG@50             0.597562\n",
       "                             P@10[>=1]           0.657644\n",
       "                XGB.1k       NDCG@10             0.487220\n",
       "                             NDCG@50             0.595683\n",
       "                             P@10[>=1]           0.682194"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_50 = NDCG(cutoff=50)\n",
    "prec_10 = Precision(cutoff=10)\n",
    "\n",
    "perf = model_performance(datasets=[msn1], \n",
    "                         models=[qr_1K, xgb_1K], \n",
    "                         metrics=[ndcg_10, ndcg_50, prec_10])\n",
    "perf.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a4afdedea141ffa763901dda36297c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571e6bd6ebef4cd1b5c58c228ddac3e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc0d048b2694157827109511cc4d96c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b57dfd1fb074073bb8fccc4e2542422"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Statistical Significance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>metric</th>\n",
       "      <th>p-value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MSN10K - Fold 1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">NDCG@10</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.03152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.06507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">NDCG@50</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.14784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.29726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">P@10[&gt;=1]</th>\n",
       "      <th>one-sided</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two-sided</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Statistical Significance\n",
       "dataset         metric    p-value                            \n",
       "MSN10K - Fold 1 NDCG@10   one-sided                   0.03152\n",
       "                          two-sided                   0.06507\n",
       "                NDCG@50   one-sided                   0.14784\n",
       "                          two-sided                   0.29726\n",
       "                P@10[>=1] one-sided                   0.00000\n",
       "                          two-sided                   0.00000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_sig = statistical_significance(datasets=[msn1],\n",
    "                                    model_a=qr_1K, model_b=xgb_1K, \n",
    "                                    metrics=[ndcg_10, ndcg_50, prec_10],\n",
    "                                    n_perm=100000 )\n",
    "stat_sig.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above experiments, we see no sigificant differences in terms of NDCG@50, but the preformance of XGBoost is better and statistically significant in terms of Precision@10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias vs. Variance decomposition\n",
    "\n",
    "The Error of a Algorithm can be decomposed in:\n",
    "$$E(A) = Bias(A) + Variance(A) + Noise(A)$$\n",
    "where:\n",
    " - Bias is how far is the model from the prediction\n",
    " - Variance is how sensitive (how changes) the prediction with different training sets (overfitting)\n",
    " - Noise is the irreducible error in the dataset (learner independent)\n",
    "\n",
    "**RankEval** supports the computation of the bias vs. variance decomposition of the error.\n",
    "The approach used is based on the works of [Webb05] and [Dom05]. As in other works, we hereinafter assume noise is absent.\n",
    "\n",
    "RankEval allows to decompose the errore according to a given user provided (IR) quality metric as follows.\n",
    "\n",
    "Each instance of the dataset is scored *L* times.\n",
    "A single scoring is achieved by splitting the dataset at random into\n",
    "*k* folds. Each fold is scored by the model *M* trained with the algorithm $A$ on the remainder folds.\n",
    "[Webb05] recommends the use of 2 folds.\n",
    "\n",
    "If the metric used is Mean Squared Error then the standard decomposition is used.\n",
    "The Bias for and instance *x* is defined as mean squared error of the *L* trained models\n",
    "w.r.t. the true label *y*, denoted with ${\\sf E}_{L} [M(x) - y]^2$. \n",
    "The Variance for an instance *x* is measured across the *L* trained models: \n",
    "${\\sf E}_{L} [M(x) - {\\sf E}_{L} M(x)]^2$. \n",
    "Both are averaged over all instances in the dataset.\n",
    "\n",
    "If the metric is any of the IR quality measures, we resort to the bias variance\n",
    "decomposition of the mean squared error of the given metric w.r.t. its ideal value,\n",
    "e.g., for the case of NDCG, ${\\sf E}_{L} [1 - {\\sf NDCG}]^2$. \n",
    "Recall that, a formal Bias/Variance decomposition was not proposed yet.\n",
    "\n",
    "##### References\n",
    " - [Webb05] Webb, Geoffrey I., and Paul Conilione. \"Estimating bias and variance from data.\" Pre-publication manuscript (2005).\n",
    " - [Dom05] Domingos P. A. \"Unified bias-variance decomposition.\" In Proceedings of 17th International Conference on Machine Learning 2000 (pp. 231-238)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset and define metrics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.analysis.statistical import bias_variance\n",
    "\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.metrics import NDCG\n",
    "from rankeval.metrics import MSE\n",
    "\n",
    "# load data\n",
    "dataset_container = load_dataset(dataset_name='msn10k', \n",
    "                                fold='1', \n",
    "                                download_if_missing=True, \n",
    "                                force_download=False, \n",
    "                                with_models=False)\n",
    "\n",
    "# we use the test dataset here\n",
    "msn1 = dataset_container.train_dataset\n",
    "msn1.name = \"MSN10K - Fold 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the algorithm of wich we want to measure its bias/variance decomposition\n",
    "\n",
    "The Bias/Variancs decomposition is a measure of a given algorithm with given parameters. Recall that RankEval needs to repeatedly train and evaluate models learnt by the given algorithm. To do so, we define a wrapper function to be used by RankEval with the following parameters:\n",
    " - `train_X`: numpy.ndarray storing a 2-D matrix of size num_docs x num_features\n",
    " - `train_Y`: numpy.ndarray storing a vector of document's relevance labels\n",
    " - `train_q`: numpy.ndarray storing a vector of query lengths\n",
    " - `test_X`: numpy.ndarray as for `train_X`\n",
    "Such wrapper function trains a new model on `train_X`, `train_Y`, `train_q`, then used to score `test_X`.\n",
    "An `numpy.ndarray` with such scores is returned.\n",
    "\n",
    "In the example below we use LightGBM, for which we define a two wrapper function for training forests of 100 trees and with eithr 32 (`lgbm_small_wrapper`) or 64 (`lgbm_large_wrapper`) leaves each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "def lgbm_algo(trees, leaves, train_X, train_Y, train_q, test_X):\n",
    "    params = {'num_leaves': leaves, 'objective':'lambdarank', 'learning_rate': 0.05}\n",
    "\n",
    "    training = lightgbm.Dataset(data=train_X, label=train_Y, group=train_q)\n",
    "    \n",
    "    bst = lightgbm.train(params, training, num_boost_round=trees)\n",
    "    \n",
    "    return bst.predict(test_X)\n",
    "\n",
    "def lgbm_small(train_X, train_Y, train_q, test_X):\n",
    "    return lgbm_algo(100, 16, train_X, train_Y, train_q, test_X)\n",
    "\n",
    "def lgbm_large(train_X, train_Y, train_q, test_X):\n",
    "    return lgbm_algo(100, 128, train_X, train_Y, train_q, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the bias/variance decomposition\n",
    "\n",
    "The function `bias_variance` returns an `array.DataArray` containing the bias/variance decomposition of the error for the given datasets, algorithms and metrics.\n",
    "\n",
    "Below the bias variance decomposition for the two LightGBM models for both the MSE and NDCG@10 losses. \n",
    "\n",
    "Interestingly, regarding the MSE loss, the Variance increases by a factor of 2 with the larger model including 128 leaves. Also note that the smaller model is the best performing in terms of MSE but not in terms of NDCG. This is because the model was trained by optimizing the Lambda-Rank loss and not the MSE: predicted scores may deviate from the target labels, but the resulting ranking is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of rankeval.analysis.statistical failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name MSE\n",
      "]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69eff480589c4e82aa1cf3e27d8a87ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fd8f293cc34a4eb4340ba70f447d79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1cf4db34ee40a9b627d7ea85f83423"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3195f94b98544a799d8844565812211d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-af89d2719712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                    \u001b[0malgos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlgbm_small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgbm_large\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#[\"MSE\", ndcg_10],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                    L=2, k=2)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rankeval/rankeval/notebooks/rankeval/analysis/statistical.py\u001b[0m in \u001b[0;36mbias_variance\u001b[0;34m(datasets, algos, metrics, L, k)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_multi_kfold_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rankeval/rankeval/notebooks/rankeval/analysis/statistical.py\u001b[0m in \u001b[0;36m_multi_kfold_scoring\u001b[0;34m(dataset, algo, L, k)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kfold_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rankeval/rankeval/notebooks/rankeval/analysis/statistical.py\u001b[0m in \u001b[0;36m_kfold_scoring\u001b[0;34m(dataset, k, algo)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mtrain_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffled_qid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# get algorithm predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         fold_scores = algo(dataset.X[train_rows], dataset.y[train_rows], q_lens[train_q],\n\u001b[0m\u001b[1;32m    197\u001b[0m                            dataset.X[test_rows])\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# update scores for the current fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-241ef7d1dd79>\u001b[0m in \u001b[0;36mlgbm_small\u001b[0;34m(train_X, train_Y, train_q, test_X)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlgbm_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlgbm_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlgbm_large\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-241ef7d1dd79>\u001b[0m in \u001b[0;36mlgbm_algo\u001b[0;34m(trees, leaves, train_X, train_Y, train_q, test_X)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlightgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlightgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/lightgbm/engine.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    183\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/lightgbm/basic.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1371\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1372\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1374\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bv = bias_variance(datasets=[msn1], \n",
    "                   algos=[lgbm_small, lgbm_large], \n",
    "                   metrics=[MSE()], #[\"MSE\", ndcg_10], \n",
    "                   L=2, k=2)\n",
    "bv.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
