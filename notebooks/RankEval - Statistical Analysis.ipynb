{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance\n",
    "\n",
    "... on going ...\n",
    "\n",
    "by reading a few papers [cite] the methodology is as follows...\n",
    "\n",
    "The Loss of a Algorithm (not of a model!) can be decomposed in:\n",
    "$$L(A) = Bias(A) + Variance(A) + Noise(A)$$\n",
    "where:\n",
    " - Bias is how far is the model from the prediction\n",
    " - Variance is how sensitive (how changes) the prediction with different training sets (overfitting)\n",
    " - Noise is the irreducible error in the dataset (learner independent)\n",
    "\n",
    "Given a dataset $D$, given a number of iteration $l$, we need to produce $l$ score predictions\n",
    "for each element in $D$ with $l$ different models. If the loss function $L$ is MSE then everything is clear.\n",
    "If the loss function is NDCG, no previous work.\n",
    "\n",
    "  0. Shuffle $D$ and create two halves $D_1$ and $D_2$ (this should be query-aware with NDCG)\n",
    "  0. learn a model from $D_1$ and apply it to $D_2$\n",
    "  0. learn a model from $D_2$ and apply it to $D_1$\n",
    "  0. store the scores produced so far\n",
    "  0. Repeat all of the above $l$ times\n",
    "  0. for each document/query compute the average prediction by averagin the $l$ stored so far (for ranking this is actually not defined, we may average predictions and then rank accordingly)\n",
    "  0. Bias of a query/document is computed as the loss between the ideal prediction and the average prediction\n",
    "  0. Average above for every query/document to obtain final Bias\n",
    "  0. Variance of a query/document is computed as the average loss between the average prediction and every of the $l$ predictions\n",
    "  0. Average above for every query/document to obtain final Variance\n",
    " \n",
    "Usually Noise is assumed to be 0 as it is impossible to estimated it.\n",
    "As a consequence, ometimes Variance is defined as $L(A)-Bias(A)$.\n",
    "\n",
    "In every work, $L(A)$ should be equal to $Bias(A) + Variance(A)$. Is this true for NDCG? probably not ...\n",
    "\n",
    "In principle, we may provide just the Loss and the Bias, and let the user compute variance by difference.\n",
    "\n",
    "We might probalby solve the ranking problem, by saying that L is not 1-NDCG, but rather (1-NDCG)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from rankeval.core.dataset import Dataset\n",
    "from rankeval.core.metrics import NDCG\n",
    "from rankeval.core.metrics.rmse import RMSE\n",
    "from rankeval.core.metrics.metric import Metric\n",
    "from rankeval.analysis.statistical import bias_variance\n",
    "import numpy as np\n",
    "import math\n",
    "import lightgbm\n",
    "#from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.load(\"/Users/claudio/docs/LAVORO/coding/ltr/QuickScorer/debug_data/msn1.fold1.test.5k.txt\", name=\"msn1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgbm_algo(train_X, train_Y, train_q, test_X):\n",
    "    params = {'num_leaves': 128, 'objective':'lambdarank',\n",
    "             'learning_rate': 0.01, 'max_bin': 1024}\n",
    "\n",
    "    training = lightgbm.Dataset(data=train_X, label=train_Y, group=train_q)\n",
    "    \n",
    "    bst = lightgbm.train(params, training, num_boost_round=10)\n",
    "    \n",
    "    # check the number of trees and other params is correct\n",
    "    # print ( len(bst.dump_model()[u'tree_info']) )\n",
    "    \n",
    "    return bst.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " + Dataset scoring 0 of 5\n",
      " - Processing fold 0 of 3\n",
      " - Processing fold 1 of 3\n",
      " - Processing fold 2 of 3\n",
      " + Dataset scoring 1 of 5\n",
      " - Processing fold 0 of 3\n",
      " - Processing fold 1 of 3\n",
      " - Processing fold 2 of 3\n",
      " + Dataset scoring 2 of 5\n",
      " - Processing fold 0 of 3\n",
      " - Processing fold 1 of 3\n",
      " - Processing fold 2 of 3\n",
      " + Dataset scoring 3 of 5\n",
      " - Processing fold 0 of 3\n",
      " - Processing fold 1 of 3\n",
      " - Processing fold 2 of 3\n",
      " + Dataset scoring 4 of 5\n",
      " - Processing fold 0 of 3\n",
      " - Processing fold 1 of 3\n",
      " - Processing fold 2 of 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0553246, 1.0552039, 0.00012066727)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_variance(dataset, algo=lgbm_algo, metric=\"mse\", L=5, k=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' + Dataset scoring', 0, 'of', 10)\n",
      "(' + Dataset scoring', 1, 'of', 10)\n",
      "(' + Dataset scoring', 2, 'of', 10)\n",
      "(' + Dataset scoring', 3, 'of', 10)\n",
      "(' + Dataset scoring', 4, 'of', 10)\n",
      "(' + Dataset scoring', 5, 'of', 10)\n",
      "(' + Dataset scoring', 6, 'of', 10)\n",
      "(' + Dataset scoring', 7, 'of', 10)\n",
      "(' + Dataset scoring', 8, 'of', 10)\n",
      "(' + Dataset scoring', 9, 'of', 10)\n"
     ]
    }
   ],
   "source": [
    "a = bias_variance(dataset, algo=lgbm_algo, metric=NDCG(cutoff=10), L=10, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.42988306, 0.41835901, 0.011524014)\n"
     ]
    }
   ],
   "source": [
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = NDCG(cutoff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
